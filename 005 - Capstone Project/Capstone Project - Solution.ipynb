{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    " This POC has been set up in order to see the possible business value that can be extracted from insights on the patterns of tourist visiting the USA. \n",
    " The goal is to asses:\n",
    " - Easiness and value of business insights extraction \n",
    " - Fesability in therms of human and technical capital \n",
    " - Issues encountered and possible remediation techniques\n",
    "\n",
    "Being part fo an interdisciplinary agile team, you have been charged during the spring to create a simple data pipeline transformation.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pyspark.sql as pysql\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "As initial focus, the major driving forces of this POC will be studying simple implications between:\n",
    "- __Tourist__: characteristics and idiosincrasies  \n",
    "- __Weather__: during the stayed moment (not taking into consideration _second order thoughts_,i.e. which weather would I think it will make __when__ I decide to purchase the flight)\n",
    "- __Cities__: characteristics\n",
    "\n",
    "#### Data Gathered\n",
    " - __I94 Immigration Data__: This [data](https://travel.trade.gov/research/reports/i94/historical/2016.html) comes from the US National Tourism and Trade Office. \n",
    " - __World Temperature Data__: This [data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) informs us about the surface temperature by city.\n",
    " - __U.S: Demographic Data__: This [data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) ifnroms us about US cities demographics.\n",
    " - __Airport Code Table__: This [data](https://datahub.io/core/airport-codes#data), gives us detailed information of the airport codes and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airport Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes = pd.read_csv('airport-codes_csv.csv')\n",
    "airport_codes = airport_codes[airport_codes['iso_country']=='US']\n",
    "airport_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- indent: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: float (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_schema = StructType([\n",
    "    StructField(\"indent\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"elevation_ft\", FloatType()),\n",
    "    StructField(\"continent\", StringType()),\n",
    "    StructField(\"iso_country\", StringType()),\n",
    "    StructField(\"iso_region\", StringType()),\n",
    "    StructField(\"municipality\", StringType()),\n",
    "    StructField(\"gps_code\", StringType()),\n",
    "    StructField(\"iata_code\", StringType()),\n",
    "    StructField(\"local_code\", StringType()),\n",
    "    StructField(\"coordinates\", StringType())\n",
    "])\n",
    "\n",
    "airport_codes = spark.read.csv('airport-codes_csv.csv', schema=airport_schema, header=True, mode='DROPMALFORMED')\n",
    "airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(indent='00A', type='heliport', name='Total Rf Heliport', elevation_ft=11.0, continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125')]\n"
     ]
    }
   ],
   "source": [
    "print(airport_codes.limit(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### U.S. City Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_city  = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "us_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: float (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: float (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_schema = StructType([\n",
    "    StructField(\"City\", StringType()),\n",
    "    StructField(\"State\", StringType()),\n",
    "    StructField(\"Median Age\", FloatType()),\n",
    "    StructField(\"Male Population\", IntegerType()),\n",
    "    StructField(\"Female Population\", IntegerType()),\n",
    "    StructField(\"Total\", IntegerType()),\n",
    "    StructField(\"Number of Veterans\", IntegerType()),\n",
    "    StructField(\"Foreign-born\", IntegerType()),\n",
    "    StructField(\"Average Household Size\", FloatType()),\n",
    "    StructField(\"State Code\", StringType()),\n",
    "    StructField(\"Race\", StringType()),\n",
    "    StructField(\"Count\", IntegerType())\n",
    "])\n",
    "\n",
    "us_city = spark.read.csv('us-cities-demographics.csv',  sep=';', schema=city_schema, header=True, mode='DROPMALFORMED')\n",
    "us_city.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(City='Silver Spring', State='Maryland', Median Age=33.79999923706055, Male Population=40601, Female Population=41862, Total=82463, Number of Veterans=1562, Foreign-born=30908, Average Household Size=2.5999999046325684, State Code='MD', Race='Hispanic or Latino', Count=25924)]\n"
     ]
    }
   ],
   "source": [
    "print(us_city.limit(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature  = pd.read_csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "temperature.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: float (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: float (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_schema = StructType([\n",
    "    StructField(\"dt\", TimestampType()),\n",
    "    StructField(\"AverageTemperature\", FloatType()),\n",
    "    StructField(\"AverageTemperatureUncertainty\", FloatType()),\n",
    "    StructField(\"City\", StringType()),\n",
    "    StructField(\"Country\", StringType()),\n",
    "    StructField(\"Latitude\", StringType()),\n",
    "    StructField(\"Longitude\", StringType()),\n",
    "])\n",
    "\n",
    "temperature = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv', schema=temperature_schema, header=True, mode='DROPMALFORMED')\n",
    "temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dt=datetime.datetime(1743, 11, 1, 0, 0), AverageTemperature=6.067999839782715, AverageTemperatureUncertainty=1.7369999885559082, City='Århus', Country='Denmark', Latitude='57.05N', Longitude='10.33E')]\n"
     ]
    }
   ],
   "source": [
    "print(temperature.limit(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "\n",
       "   i94mode i94addr    ...     entdepu  matflag  biryear   dtaddto  gender  \\\n",
       "0      1.0      HI    ...         NaN        M   1955.0  07202016       F   \n",
       "1      1.0      TX    ...         NaN        M   1990.0  10222016       M   \n",
       "2      1.0      FL    ...         NaN        M   1940.0  07052016       M   \n",
       "3      1.0      CA    ...         NaN        M   1991.0  10272016       M   \n",
       "4      3.0      NY    ...         NaN        M   1997.0  07042016       F   \n",
       "\n",
       "  insnum airline        admnum  fltno  visatype  \n",
       "0    NaN      JL  5.658267e+10  00782        WT  \n",
       "1    NaN     *GA  9.436200e+10  XBLNG        B2  \n",
       "2    NaN      LH  5.578047e+10  00464        WT  \n",
       "3    NaN      QR  9.478970e+10  00739        B2  \n",
       "4    NaN     NaN  4.232257e+10   LAND        WT  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration = pd.read_csv('immigration_data_sample.csv')\n",
    "immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: double (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_schema = StructType([\n",
    "    StructField(\"cicid\", DoubleType()),\n",
    "    StructField(\"i94yr\", DoubleType()),\n",
    "    StructField(\"i94mon\", DoubleType()),\n",
    "    StructField(\"i94cit\", DoubleType()),\n",
    "    StructField(\"i94res\", DoubleType()),\n",
    "    StructField(\"i94port\", StringType()),\n",
    "    StructField(\"arrdate\", DoubleType()),\n",
    "    StructField(\"i94mode\", DoubleType()),\n",
    "    StructField(\"i94addr\", StringType()),\n",
    "    StructField(\"depdate\", DoubleType()),\n",
    "    StructField(\"i94bir\", DoubleType()),\n",
    "    StructField(\"i94visa\", DoubleType()),\n",
    "    StructField(\"count\", DoubleType()),\n",
    "    StructField(\"dtadfile\", DoubleType()),\n",
    "    StructField(\"visapost\", StringType()),\n",
    "    StructField(\"occup\", StringType()),\n",
    "    StructField(\"entdepa\", StringType()),\n",
    "    StructField(\"entdepd\", StringType()),\n",
    "    StructField(\"entdepu\", StringType()),\n",
    "    StructField(\"matflag\", StringType()),\n",
    "    StructField(\"biryear\", DoubleType()),\n",
    "    StructField(\"dtaddto\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"insnum\", StringType()),\n",
    "    StructField(\"airline\", StringType()),\n",
    "    StructField(\"admnum\", DoubleType()),\n",
    "    StructField(\"fltno\", StringType()),\n",
    "    StructField(\"visatype\", StringType()),\n",
    "])\n",
    "\n",
    "immigration = None\n",
    "immigration_columns = None\n",
    "for file in os.listdir('../../data/18-83510-I94-Data-2016'):\n",
    "        if not immigration:\n",
    "            immigration = spark.read.load(path='../../data/18-83510-I94-Data-2016/' + file, format='com.github.saurfang.sas.spark', schema=immigration_schema, header=True, mode='DROPMALFORMED')\n",
    "            immigration_columns = immigration.columns\n",
    "        else:\n",
    "            immigration_temp = spark.read.load(path='../../data/18-83510-I94-Data-2016/' + file, format='com.github.saurfang.sas.spark', schema=immigration_schema, header=True, mode='DROPMALFORMED')\n",
    "            immigration_temp = immigration_temp.select(immigration_columns)\n",
    "            immigration= immigration.unionByName(immigration_temp)\n",
    "immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(cicid=6.0, i94yr=2016.0, i94mon=4.0, i94cit=692.0, i94res=692.0, i94port='XXX', arrdate=20573.0, i94mode=None, i94addr=None, depdate=None, i94bir=37.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu='U', matflag=None, biryear=1979.0, dtaddto='10282016', gender=None, insnum=None, airline=None, admnum=1897628485.0, fltno=None, visatype='B2')]\n"
     ]
    }
   ],
   "source": [
    "print(immigration.limit(1).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Data exploration is to be hold at an absolute minimum. Due to the \"continuous nature\" of data pipelines, it is not performant to establish manual controls; the best approach is to establish automatic data cleaning systems.\n",
    "This pipeline prepares the golden datasets and its derived tables, extensive data exploration is to be held once it is produced.\n",
    "\n",
    "\n",
    "#### Cleaning Steps\n",
    "##### Airport Codes\n",
    "For the `airport codes` table, we are solely interested in the _bridge information_ it contains relating the __airport codes__ to the diferent __cities__ names. For this data just a simple string processing to remove any \n",
    "caps, to remove the likelihood of not having successful joins by `caps-missmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select required columns\n",
    "airport_codes = airport_codes.select('iso_country', 'municipality', 'iata_code')\n",
    "\n",
    "# Small string processing\n",
    "airport_codes = airport_codes.withColumn('iso_country', F.lower(airport_codes.iso_country))\\\n",
    "                             .withColumn('municipality', F.lower(airport_codes.municipality))\\\n",
    "                             .withColumn('iata_code', F.lower(airport_codes.iata_code))\n",
    "\n",
    "# NAs and duplicates processing\n",
    "airport_codes = airport_codes.dropna(how='any')\n",
    "airport_codes = airport_codes.dropDuplicates()\n",
    "\n",
    "# Filtering of data to specific scope\n",
    "airport_codes = airport_codes.filter(airport_codes.iso_country == 'US')\n",
    "\n",
    "# Remove unnecessary columns\n",
    "airport_codes = airport_codes.drop('iso_country')\n",
    "\n",
    "# Renaming to decided conventions\n",
    "airport_codes = airport_codes.withColumnRenamed('municipality', 'city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### U.S. City Demographic\n",
    "The `U.S. City Demographic` contains much more useful information for the expected analysis, some columns are removed due to being redundant and other due to have non scientific base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select required columns\n",
    "us_city = us_city.select('City', 'Median Age', 'Male Population', 'Female Population', 'Total', 'Foreign-born', 'Average Household Size')\n",
    "\n",
    "# Renaming to decided conventions\n",
    "us_city = us_city.withColumnRenamed('City', 'city')\\\n",
    "                 .withColumnRenamed('Median Age', 'city_age')\\\n",
    "                 .withColumnRenamed('Male Population', 'population_male')\\\n",
    "                 .withColumnRenamed('Female Population', 'population_female')\\\n",
    "                 .withColumnRenamed('Total', 'population')\\\n",
    "                 .withColumnRenamed('Foreign-born', 'population_no_native')\\\n",
    "                 .withColumnRenamed('Average Household Size', 'house_size')\n",
    "\n",
    "# NAs and duplicates processing\n",
    "us_city = us_city.dropna(how='any')\n",
    "us_city = us_city.dropDuplicates()\n",
    "\n",
    "# Small string processing\n",
    "us_city = us_city.withColumn('city', F.lower(us_city.city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### World Temperature\n",
    " For the world temperature we will select just the temperature information related data and the cities as key to union with other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select required columns\n",
    "temperature = temperature.select('dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'City', 'Country')\n",
    "\n",
    "# Renaming to decided conventions\n",
    "temperature = temperature.withColumnRenamed('dt', 'datetime')\\\n",
    "                 .withColumnRenamed('AverageTemperature', 'temperature')\\\n",
    "                 .withColumnRenamed('AverageTemperatureUncertainty', 'temperature_uncertainty')\\\n",
    "                 .withColumnRenamed('City', 'city')\\\n",
    "                 .withColumnRenamed('Country', 'country')\n",
    "\n",
    "# Filtering of data to specific scope\n",
    "temperature = temperature.filter(temperature.country == 'United States')\n",
    "\n",
    "min_date = datetime.datetime(2016, 1, 1, 0, 0)\n",
    "max_date = datetime.datetime(2017, 1, 1, 0, 0)\n",
    "temperature = temperature.filter(temperature.datetime >= min_date)\\\n",
    "                         .filter(temperature.datetime < max_date)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "temperature = temperature.drop('country')\n",
    "\n",
    "# NAs and duplicates processing\n",
    "temperature = temperature.dropna(how='any')\n",
    "temperature = temperature.dropDuplicates()\n",
    "\n",
    "# Small string processing\n",
    "temperature = temperature.withColumn('city', F.lower(temperature.city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### I94 Immigration Data\n",
    "The immigration data, contains many data relating the travelers and `border security`relevant features.\n",
    "This dataset analysis of the different fields remains pretty evasive, as no source of the data fields is\n",
    "provided; neither it is ´obvious'(to the developer) to understand their meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select required columns\n",
    "immigration = immigration.select('i94yr', 'i94mon', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'biryear', 'gender', 'airline', 'visatype')\n",
    "\n",
    "# Renaming to decided conventions\n",
    "immigration = immigration.withColumnRenamed('i94yr', 'year')\\\n",
    "                         .withColumnRenamed('i94mon', 'month')\\\n",
    "                         .withColumnRenamed('i94port', 'iata_code')\\\n",
    "                         .withColumnRenamed('arrdate', 'arrival_date')\\\n",
    "                         .withColumnRenamed('i94mode', 'mode')\\\n",
    "                         .withColumnRenamed('i94addr', 'state_address')\\\n",
    "                         .withColumnRenamed('depdate', 'departure_date')\\\n",
    "                         .withColumnRenamed('i94bir', 'age')\\\n",
    "                         .withColumnRenamed('i94visa', 'visa')\\\n",
    "                         .withColumnRenamed('biryear', 'year_born')\\\n",
    "                         .withColumnRenamed('visatype', 'visa_type')\n",
    "\n",
    "# Filtering of data to specific scope\n",
    "immigration = immigration.filter(immigration.year == 2016)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "immigration = immigration.drop('year')\n",
    "\n",
    "# NAs and duplicates processing\n",
    "immigration = immigration.dropna(how='any')\n",
    "immigration = immigration.dropDuplicates()\n",
    "\n",
    "# Small string processing\n",
    "immigration = immigration.withColumn('iata_code', F.lower(immigration.iata_code))\\\n",
    "                         .withColumn('state_address', F.lower(immigration.state_address))\\\n",
    "                         .withColumn('gender', F.lower(immigration.gender))\\\n",
    "                         .withColumn('airline', F.lower(immigration.airline))\\\n",
    "                         .withColumn('visa_type', F.lower(immigration.visa_type))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The choosen data model joins the different tables, their structure is:\n",
    "- temperature: this table is a facts table\n",
    "- immigration: this is a fact table\n",
    "- us_city: this is a dimentional table\n",
    "- airport_codes: this is a dimentional table\n",
    "\n",
    "Having not a _unique_ facts table along, to create a star diagram structure;\n",
    "we can still create something that resembles it by having as central part of \n",
    "our structure the `immigrations`table.\n",
    "\n",
    "A code representation of the structure would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'iata_code',\n",
       " 'month',\n",
       " 'arrival_date',\n",
       " 'mode',\n",
       " 'state_address',\n",
       " 'departure_date',\n",
       " 'age',\n",
       " 'visa',\n",
       " 'year_born',\n",
       " 'gender',\n",
       " 'airline',\n",
       " 'visa_type',\n",
       " 'datetime',\n",
       " 'temperature',\n",
       " 'temperature_uncertainty',\n",
       " 'city_age',\n",
       " 'population_male',\n",
       " 'population_female',\n",
       " 'population',\n",
       " 'population_no_native',\n",
       " 'house_size']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = immigration.join(airport_codes, \"iata_code\")\\\n",
    "                     .join(temperature, \"city\")\\\n",
    "                     .join(us_city, \"city\")\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This model has been choosen as a __vanilla model__, it makes no assumptions on the kind of query/analysis\n",
    "that will be processed, neither on the type of technical access specifications (in therms of users and/or\n",
    "SLAs). The data is simply all available and all together.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The proposed pipeline is synthesized in the following parts:\n",
    "1. __Landing__: data loading on the spark sessions\n",
    "2. __Staging__: data aggregation in a single Dataframe (in this case just for ´immigration´ data)\n",
    "3. __Data Processing__: in this steps we will devise several sub-steps\n",
    "    +  __Golden datasets__: golden datasets creation consisted on the step of data cleaning for each specific data source (still not merged together)\n",
    "    + __Derived datasets__: in this case there is just one single derived dataset, with all information available (data model _pseudo-star_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Data model creation is performed in the above section __3.1__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data quality checks can be performed in two ways: \n",
    "- __Data Content__: checks related to ensure we are not introducing in a given field, values we can ensure (by the typology of the source) to be incorrect. This can be:\n",
    "    + _Idependent data values_ based, meaning we know each specific value must enforce certain parameters (age > 0 ...) and it is independent of the others values\n",
    "    + _Dependent data values_ based, on an attribute's values on its ensemble, e.g. big deviations of the mean value (above 1.5 standard deviation) for a specific attribute from one ingestion to the previous one\n",
    "- __Metadata__: checks related with the data structure and integrity maintenance as it is moving through the pipeline, this checks include:\n",
    "    + Row counts\n",
    "    + Checksums\n",
    "    + ...\n",
    "- __Pipeline__: checks create to ensure the different data pipelines code and transformations really perform as expected:\n",
    "    + Database integrity checks\n",
    "    + Unit tests\n",
    "    + Behaviour tests\n",
    "    + ...\n",
    "\n",
    "\n",
    "On the different kind of checks applicable to this `data pipeline`, the scope will be reduced to _data content_ ones.\n",
    "The _Metadata_ type is not going to be considered as we are working on a single Spark instance, and it is to be considered\n",
    "that on this environment this king of __data corruption__ is not likely to ocurre (check already developed into the framework).\n",
    "As fo the _pipeline_ type, this is not a proper pipeline encompassing several systems on different machines, hence they are not\n",
    "going to be studied; still the ones derived from __improper coding__ is being review by the developer while creating this \n",
    "Jupyter notebook.\n",
    "\n",
    "##### Airport Codes\n",
    "Has no room for verification, vis a vis of the data it contains:\n",
    "- city\n",
    "- iata code\n",
    "\n",
    "\n",
    "##### U.S. City Demographic\n",
    "Can have some data checks related to the `population` and `age` attributes:\n",
    "- city\n",
    "- city_age\n",
    "- male population\n",
    "- female population\n",
    "- population\n",
    "- non native population\n",
    "- house size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Age\n",
    "us_city = us_city.filter(us_city.city_age >= 0)\\\n",
    "                 .filter(us_city.city_age <= 130)\n",
    "\n",
    "# Population\n",
    "us_city = us_city.withColumn('check_failed', us_city.population != (us_city.population_male + us_city.population_female))\n",
    "us_city = us_city.withColumn('check_failed', (us_city.check_failed) | (us_city.population < us_city.population_no_native))\n",
    "\n",
    "us_city = us_city.filter(us_city.check_failed != True)\n",
    "us_city = us_city.drop('check_failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### World Temperature\n",
    " This table can have data checks related to the temperature vaues; the ´datetime´ have already been enforced by the \n",
    " conversion procedure:\n",
    " - datetime\n",
    " - temperature\n",
    " - temperature_uncertainty\n",
    " - city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Temperature\n",
    "temperature = temperature.filter(temperature.temperature > -60)\\\n",
    "                         .filter(temperature.temperature < 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### I94 Immigration Data\n",
    "Can have some checks related to the `dates` of arrival and departure and `age` of the arriving passengers:\n",
    "- month\n",
    "- iata_code\n",
    "- arrival_date\n",
    "- mode\n",
    "- state_address\n",
    "- departure_date\n",
    "- age\n",
    "- visa\n",
    "- year_born\n",
    "- gender\n",
    "- airline\n",
    "- visa_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Date\n",
    "immigration = immigration.filter(immigration.month >= 1)\\\n",
    "                         .filter(immigration.month <= 12)\\\n",
    "                         .filter(immigration.arrival_date >= immigration.departure_date)\n",
    "\n",
    "# Age\n",
    "immigration = immigration.filter(immigration.age >= 0)\\\n",
    "                         .filter(immigration.age <= 130)\\\n",
    "                         .filter(immigration.year_born >= 1890)\\\n",
    "                         .filter(immigration.year_born <= 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "The data dictionary for `dataset` table data model, is:\n",
    "- city: city name; _source `us_city` and `airport_codes`_\n",
    "- iata_code: airport iata codes;  _source `airport_codes`_\n",
    "- month: month of the year; _source `immigration`_\n",
    "- arrival_date: visitor arrival date (format unknown and no information provided on it); _source `immigration`_\n",
    "- mode: visa modality (format unknown and no information provided on it); _source `immigration`_\n",
    "- state_address: US state from the address provided; _source `immigration`_ \n",
    "- departure_date: visitor departure date (format unknown and no information provided on it); _source `immigration`_\n",
    "- age: visitors age; _source `immigration`_\n",
    "- visa: visitor's visa;  _source `immigration`_\n",
    "- year_born: visitor's year of birth; _source `immigration`_\n",
    "- gender: visitor's gender; _source `immigration`_\n",
    "- airline: visitor's airline; _source `immigration`_\n",
    "- visa_type: visitor's visa type;  _source `immigration`_\n",
    "- datetime: datetime of the temperature information;  _source `temperature`_\n",
    "- temperature: global average land temperature in celsius;  _source `temperature`_\n",
    "- temperature_uncertainty: the 95% confidence interval around the average;  _source `temperature`_\n",
    "- city_age: city's median age; _source `us_city`_\n",
    "- population_male: city's median male age; _source `us_city`_\n",
    "- population_female: city's median female age; _source `us_city`_\n",
    "- population: city's population; _source `us_city`_\n",
    "- population_no_native: city's non-native population; _source `us_city`_\n",
    "- house_size: city's median house size (unit of measure not specified, may be __rooms__); _source `us_city`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'iata_code',\n",
       " 'arrival_date',\n",
       " 'mode',\n",
       " 'state_address',\n",
       " 'departure_date',\n",
       " 'age',\n",
       " 'visa',\n",
       " 'year_born',\n",
       " 'gender',\n",
       " 'airline',\n",
       " 'visa_type']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city', 'iata_code']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'city_age',\n",
       " 'population_male',\n",
       " 'population_female',\n",
       " 'population',\n",
       " 'population_no_native',\n",
       " 'house_size']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_city.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datetime', 'temperature', 'temperature_uncertainty', 'city']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'iata_code',\n",
       " 'month',\n",
       " 'arrival_date',\n",
       " 'mode',\n",
       " 'state_address',\n",
       " 'departure_date',\n",
       " 'age',\n",
       " 'visa',\n",
       " 'year_born',\n",
       " 'gender',\n",
       " 'airline',\n",
       " 'visa_type',\n",
       " 'datetime',\n",
       " 'temperature',\n",
       " 'temperature_uncertainty',\n",
       " 'city_age',\n",
       " 'population_male',\n",
       " 'population_female',\n",
       " 'population',\n",
       " 'population_no_native',\n",
       " 'house_size']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Tools and Technologies rational\n",
    "This project is a `toy-project`. Having working experience on real data engineering projects, this involve lots of stake holders and departments, alas of many different technology stacks.\n",
    "Hence the selection of doing it __solely__ on _PySpark_ was motivated due t the fact that replicating it like in _\"in real life\"_ would be too much of an overhead not so much related on\n",
    "`data engineering` skills but more into the _infrastructure_ and _business analyst_ side. \n",
    "\n",
    "\n",
    "##### Data Pipeline Scheduling\n",
    "Being a `toy-project`based on independent files load and no __permanent state__ databases, this means running it every time and saving the results of the tables\n",
    "into a file or database system. In resume the scheduling is __on demand__.\n",
    "\n",
    "\n",
    "##### Scenarios Approach\n",
    "For all of the scenarios there is a base template that will need to be fine tuned for each case specific demands. Generally speaking, the structure would consist \n",
    "of:\n",
    "- __Landing zone__: here files arrive and are stored temporary, _acts as entrance buffer. \n",
    "- __Staging zone__: here files have already passed several basic checks (data quality), in order to ensure that different data generation sources are passing data in the agreed format and data does not contain  unexpected values\n",
    "- __Golden Datasets__: in this step information of files is introduced into a structured database\n",
    "- __Derived Datasets__: in this step, users, which may be _power business analysts or data engineers/scientist_, take the golden tables and combine them in the required way for their specific business case/ project.\n",
    "\n",
    "This is a pipeline based on a file ingestion approach, there can also be direct DB pulls and/or stream (Kafka) feed. During all pipeline steps data can be saved in accordance to the code used to generate the original/derived\n",
    "datasets; to have a full data lineage with historical acces capabilities (this me be useful not only for project, also for compliance and audit purposes). The different data storage can additionally have several saving\n",
    "patterns, i.e. moving data from more available storage (and expensive) to less available one, depending on: creation_date, last modified date, ... As a schedule to oversea all of this operations I would incline myself to \n",
    "use `Apache Airflow`. \n",
    "Lastly, though not less important in a business environment it is required to have proper `data governance` and a proper `data model` of the department or company established, along with a clear `data ontology`\n",
    "\n",
    "###### Scenario 1: data explodes\n",
    "_In case there was a data explosion by 100x._\n",
    "For this case I will go to the cloud, AWS in this case is selected, as an option that allows us to scale quickly and be able to handle __peak loads__ without requiring us to pay for valley hours (and requiring by up front investments).\n",
    "- __Landing zone__: will be a comgination of `airflow workers` and `S3` for the file storage\n",
    "- __Staging zone__: will be again based on ``airflow workers` and `S3` for the file storage\n",
    "- __Golden Datasets__: will be based on an `Redshift` architecture\n",
    "- __Derived Datasets__: will be based on an `Redshift` architecture\n",
    "\n",
    "This as the main pipeline, obviously depending on the desires of enhanced __data lineage__ capabilities, the usage more capabilities is envisioned:\n",
    "- S3 glacier\n",
    "- ...\n",
    "\n",
    "###### Scenario 2: data dasboard\n",
    "_The data populates a dashboard that must be updated on a daily basis by 7am every day._\n",
    "In this case the main focus is on data being feed daily; taking the assumption of data not exploding.\n",
    "\n",
    "- __Landing zone__: will be a comgination of `airflow workers` and `S3` for the file storage\n",
    "- __Staging zone__: will be again based on ``airflow workers` and `S3` for the file storage\n",
    "- __Golden Datasets__: will be based on an `Redshift` architecture\n",
    "- __Derived Datasets__: will be based on an `Redshift` architecture\n",
    "\n",
    "As an additional point, depending on the dashboard requirements, we may change the __derived datastes__ `Redshift database` for:\n",
    "- __[Apache Impala](https://impala.apache.org/)__: Impala provides low latency and high concurrency for BI/analytic queries on Hadoop \n",
    "- __[Apache Kudu](https://kudu.apache.org/)__ : Kudu provides a combination of fast inserts/updates and efficient columnar scans to enable multiple real-time analytic workloads across a single storage layer. \n",
    "- __[Apache Kylin](https://kylin.apache.org/)__ for __OLAP__ cubes in big data environment. Kylin is able to achieve near constant query speed regardless of the ever-growing data volume. Reducing query latency from minutes to sub-second, Kylin brings online analytics back to big data.\n",
    "\n",
    "\n",
    "###### Scenario 3: data concurrency\n",
    "The database needed to be accessed by 100+ people.\n",
    "\n",
    "In this case the __strongest recommendation__ is to see the type of workload that this users will perfomr to select the best between: database, configuration  and coding. \n",
    "In general distributed databases with no master node, it may cause _bottle necks on the master node_ if many petitions are done simultaneously. \n",
    "\n",
    "Selecting as initial recommendations:\n",
    "- __[Apache HBase](https://hbase.apache.org/)__ : you need random, realtime read/write access to your Big Data. \n",
    "- __[Apache Cassandra](https://cassandra.apache.org/)__: scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
